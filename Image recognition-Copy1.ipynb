{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition of vehicle's make and model using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5 |Anaconda, Inc.| (default, Apr  7 2018, 04:52:34) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to open tensorboard in jupyter\n",
    "def TB(cleanup=False):\n",
    "    import webbrowser\n",
    "    webbrowser.open('http://127.0.1.1:6006')\n",
    "\n",
    "    !tensorboard --logdir=\"logs\"\n",
    "\n",
    "    if cleanup:\n",
    "        !rm -R logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from os.path import splitext\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import tflearn and some helpers\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/mednche/Desktop/ImageRec/Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get name of all images in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 105152 images\n",
      "Reduced to 5000 images\n"
     ]
    }
   ],
   "source": [
    "# Get all files in merged folder\n",
    "folder = \"C:/Users/mednche/Desktop/ImageRec/Test\"\n",
    "onlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "print(\"Working with {0} images\".format(len(onlyfiles)))\n",
    "\n",
    "# due to low memory available, I restricted the dataset size\n",
    "onlyfiles = onlyfiles[:5000]\n",
    "print(\"Reduced to {0} images\".format(len(onlyfiles)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the labels associated with each images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train_files: 5000\n"
     ]
    }
   ],
   "source": [
    "train_files = []\n",
    "y_train =  pd.DataFrame()\n",
    "\n",
    "# import the metadata for the cars\n",
    "data =  pd.read_csv(\"../metadata.csv\")\n",
    "\n",
    "for _file in onlyfiles:\n",
    "    # add file name to list of train_file\n",
    "    train_files.append(_file)\n",
    "    \n",
    "    # remove name extension \n",
    "    file = splitext(_file)[0]\n",
    "    \n",
    "    # get id of vehicle\n",
    "    car_id = file.split(\"_\")[0]\n",
    "    \n",
    "    # get corresponding make\n",
    "    make = data[data.id == car_id].make.tolist()[0]\n",
    "    model = data[data.id == car_id].model.tolist()[0]\n",
    "    name = car_id\n",
    "    \n",
    "    y_train = y_train.append({'name': name, 'make':make, 'model':model}, ignore_index=True)\n",
    "    \n",
    "print(\"Files in train_files: {}\".format(len(train_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check all vehicles have 16 images each (from different angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0d4b67f458bb</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              make  model\n",
       "name                     \n",
       "0d4b67f458bb     8      8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of images per vehicle\n",
    "df = y_train.groupby(y_train.name).count()\n",
    "df[df.make != 16]\n",
    "# Good, all cars have a label attached in \"metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but one vehicle does not have 16 images (only has 8 images). This is due to the subsampling of 5000 images. We'll still include that car in the training set nontheless for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that all images have a label in 'metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has got 5000 images of 313 unique vehicles\n"
     ]
    }
   ],
   "source": [
    "# number of images and vehicles\n",
    "print(\"The dataset has got {} images of {} unique vehicles\".format(len(onlyfiles), len(y_train.name.unique())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of an image and its labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(80,52.8;496x369.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mednche\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "make            Acura\n",
       "model              TL\n",
       "name     0004d4463b50\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the first image...\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "\n",
    "image = misc.imread(onlyfiles[0])\n",
    "print(plt.imshow(image, cmap=plt.cm.gray))\n",
    "\n",
    "# ...And corresponding labels (make and model)\n",
    "y_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise empty array where to add images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the first image (all images have the same size)\n",
    "from PIL import Image\n",
    "im = Image.open(folder + \"/\" + train_files[0])\n",
    "\n",
    "# set size here\n",
    "size = 450, 450 \n",
    "\n",
    "# resize\n",
    "im.thumbnail(size,Image.ANTIALIAS)\n",
    "\n",
    "# crop\n",
    "img = im.crop((0, 50, im.size[0], im.size[1]))\n",
    "\n",
    "# get final dimentions of all images\n",
    "image_width, image_height = img.size\n",
    "\n",
    "# set number of colour channels \n",
    "channels = 3\n",
    "\n",
    "# for training on coloured images\n",
    "dataset = np.ndarray(shape=(len(train_files), image_height, image_width, channels), dtype=np.float32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all images to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for _file in train_files:\n",
    "    #img = load_img(folder + \"/\" + _file)  # this is a PIL image\n",
    "    \n",
    "    \n",
    "    img = Image.open(folder + \"/\" + _file)\n",
    "    # resize\n",
    "    img.thumbnail(size,Image.ANTIALIAS)\n",
    "    # crop top of image to reduce size             \n",
    "    img = img.crop((0, 50, im.size[0], im.size[1]))\n",
    "    #plt.imshow(img, cmap=plt.cm.gray) \n",
    "    \n",
    "    # Convert to Numpy Array\n",
    "    x = np.array(img)  \n",
    "   \n",
    "    # Normalize\n",
    "    dataset[i] = x\n",
    "    i += 1\n",
    "    if i % 250 == 0:\n",
    "        print(\"{} images to array\".format(i))\n",
    "print(\"All images to array!\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode car labels (make, model, id) into numbers instead of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Associate each unique make with a number\n",
    "maketonumberdict = {}\n",
    "unique_make = y_train.make.unique() \n",
    "for i in range(len(unique_make)):\n",
    "    maketonumberdict[unique_make[i]] = i \n",
    "\n",
    "# Associate each unique model with a number                  \n",
    "modeltonumberdict = {}\n",
    "unique_model = y_train.model.unique() \n",
    "for i in range(len(unique_model)):\n",
    "    modeltonumberdict[unique_model[i]] = i\n",
    "\n",
    "# Associate each unique id string with a number                  \n",
    "idtonumberdict = {}\n",
    "unique_id = y_train.name.unique() \n",
    "for i in range(len(unique_id)):\n",
    "    idtonumberdict[unique_id[i]] = i\n",
    "\n",
    "    \n",
    "def makeAndModelToNumber(mydata, makedict, modeldict, iddict):\n",
    "    mydata_copy = mydata.copy() # make a copy otherwise the changes are made in both df\n",
    "    for i in range(len(mydata_copy['make'])):\n",
    "        mydata_copy.loc[i,'make'] = makedict[mydata_copy.loc[i,'make']]\n",
    "        mydata_copy.loc[i,'model'] = modeldict[mydata_copy.loc[i,'model']]\n",
    "        mydata_copy.loc[i,'name'] = iddict[mydata_copy.loc[i,'name']]\n",
    "    return mydata_copy\n",
    "\n",
    "clean_y_train = makeAndModelToNumber(y_train, maketonumberdict, modeltonumberdict, idtonumberdict)\n",
    "clean_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. With make, model and id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset tensorflow graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "tf.get_default_graph().get_operations()\n",
    "\n",
    "os.chdir(\"C:/Users/mednche/Desktop/ImageRec/\") # change directory to save checkpoint of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This takes around 25 mins with 5000 images on a batch size of 10 and 10 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m1762.28052\u001b[0m\u001b[0m | time: 140.801s\n",
      "| Adam | epoch: 010 | loss: 1762.28052 - acc: 0.9605 -- iter: 3990/4000\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m1759.43713\u001b[0m\u001b[0m | time: 149.502s\n",
      "| Adam | epoch: 010 | loss: 1759.43713 - acc: 0.9544 | val_loss: 1695.25523 - val_acc: 0.9460 -- iter: 4000/4000\n",
      "--\n",
      "INFO:tensorflow:/tmp/tflearn_logs/-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Wall time: 25min 46s\n",
      "INFO:tensorflow:C:\\Users\\mednche\\Desktop\\ImageRec\\carclassifier.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Network trained and saved as car-classifier.tfl!\n",
      "Accuracy of the model: [0.94599999427795411]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, clean_y_train, test_size=0.2, random_state=33)\n",
    "    \n",
    "    # trainx and trainy should be numpy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Make sure the data is normalized\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "\n",
    "    ### Define our network architecture:\n",
    "    \n",
    "    # Input is a tensor: height*width images with 3 color channels (red, green and blue) \n",
    "    \n",
    "    network = input_data(shape=[None, image_height, image_width, channels],\n",
    "                         data_preprocessing=img_prep)\n",
    "    \n",
    "    # Step 1: Convolution\n",
    "    # NB: low level recognises edges and cruves, high level recognises wheels, signs\n",
    "    network = conv_2d(network, 5, 7, activation='relu') # number of filters: 3, filter size 3 \n",
    "    # need downsampling to reduce the size of the image\n",
    "    \n",
    "    # Step 2: Max pooling\n",
    "    network = max_pool_2d(network, 2) # kernel size\n",
    "    \n",
    "    # Step 3\n",
    "    network = conv_2d(network, 5, 3, activation='relu') # number of filters: 3, filter size 3 \n",
    "    \n",
    "    # Step 4: Max pooling\n",
    "    network = max_pool_2d(network, 2) # kernel size\n",
    "                        \n",
    "    # Step 5: Fully-connected 3 node neural network\n",
    "    # Looks at all images for each class and identify the high level features in common\n",
    "    network = fully_connected(network, 3, activation='relu') # number of outputs = number of classes the model has to choose from\n",
    "    \n",
    "    # Step 6: Regression\n",
    "    network = tflearn.regression(network)\n",
    "                             \n",
    "    \n",
    "    # Wrap the network in a model object\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0, checkpoint_path='/tmp/tflearn_logs/')\n",
    "    \n",
    "    %time model.fit(X_train, y_train, validation_set= (X_test, y_test), show_metric=True, batch_size=10) # batch size of 56 or even 10 is too much\n",
    "    \n",
    "    # Save model when training is complete to a file\n",
    "    model.save(\"carclassifier.tfl\")\n",
    "    \n",
    "    print(\"Network trained and saved as carclassifier.tfl!\")\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, batch_size=10)\n",
    "    # accuracy: 0.94599999427795411\n",
    "    # loss: 363.87445\n",
    "    \n",
    "    print(\"Accuracy of the model: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\mednche\\Desktop\\ImageRec\\carclassifier.tfl\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: The name 'save_1/Const:0' refers to a Tensor which does not exist. The operation, 'save_1/Const', does not exist in the graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    929\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[1;32m--> 930\u001b[1;33m                                                     allow_operation=False)\n\u001b[0m\u001b[0;32m    931\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2414\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2455\u001b[0m                          \u001b[1;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2456\u001b[1;33m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[0;32m   2457\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'save_1/Const:0' refers to a Tensor which does not exist. The operation, 'save_1/Const', does not exist in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-bf2a187acd06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/mednche/Desktop/ImageRec/\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# change directory to save checkpoint of model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./carclassifier.tfl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, model_file, weights_only, **optargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m                      \u001b[0mcreated\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrestored\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \"\"\"\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         self.predictor = Evaluator([self.net],\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, model_file, trainable_variable_only, variable_name_map, scope_for_restore, create_new_session, verbose)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mrestorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtrainable_variable_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestorer_trainvars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1457\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    931\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[1;32m--> 933\u001b[1;33m                             + e.args[0])\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: The name 'save_1/Const:0' refers to a Tensor which does not exist. The operation, 'save_1/Const', does not exist in the graph."
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/mednche/Desktop/ImageRec/\") # change directory to save checkpoint of model\n",
    "tf.reset_default_graph()\n",
    "model.load('./carclassifier.tfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAD6CAIAAAC9JNBmAAADTUlEQVR4nO3UwQ3AIBDAsNL9Zz6xA3kgJHuCvLJm5gPg1H87AOBtNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQ2CpDYKEBiowCJjQIkNgqQ2ChAYqMAiY0CJDYKkNgoQGKjAImNAiQb00UE7rIz2ZcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=450x250 at 0x185AE278>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# revert type to uint8 to see image\n",
    "Image.fromarray(X_test[2].astype(np.uint8), 'RGB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        ..., \n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695]],\n",
       "\n",
       "       [[-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        ..., \n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695]],\n",
       "\n",
       "       [[-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        ..., \n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695],\n",
       "        [-2.79602695, -2.79602695, -2.79602695]],\n",
       "\n",
       "       ..., \n",
       "       [[-2.79630828, -2.79629588, -2.79626679],\n",
       "        [-2.79630423, -2.79629159, -2.79626274],\n",
       "        [-2.79628754, -2.7962749 , -2.79624605],\n",
       "        ..., \n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613]],\n",
       "\n",
       "       [[-2.79632878, -2.79631639, -2.79628754],\n",
       "        [-2.79630828, -2.79629588, -2.79626679],\n",
       "        [-2.79629588, -2.79628325, -2.7962544 ],\n",
       "        ..., \n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613]],\n",
       "\n",
       "       [[-2.79634142, -2.79632878, -2.79629993],\n",
       "        [-2.79632044, -2.79630828, -2.79627919],\n",
       "        [-2.79630423, -2.79629159, -2.79626274],\n",
       "        ..., \n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613],\n",
       "        [-2.79615521, -2.79613853, -2.79612613]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (250, 450, 3) for Tensor 'InputData/X:0', which has shape '(?, 250, 450, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-097791c874d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mpredict_label\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m    273\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict_builder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\helpers\\evaluator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, feed_dict)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    962\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (250, 450, 3) for Tensor 'InputData/X:0', which has shape '(?, 250, 450, 3)'"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_label(X_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (250, 450, 3) for Tensor 'InputData/X:0', which has shape '(?, 250, 450, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b5d978855860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Model prediciton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Make: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumbertomakedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumbertomodeldict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mpredict_label\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m    273\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict_builder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\helpers\\evaluator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, feed_dict)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    962\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (250, 450, 3) for Tensor 'InputData/X:0', which has shape '(?, 250, 450, 3)'"
     ]
    }
   ],
   "source": [
    "# revert the dictionnary for fast query\n",
    "numbertomakedict = {v: k for k, v in maketonumberdict.items()}\n",
    "numbertomodeldict = {v: k for k, v in modeltonumberdict.items()}\n",
    "\n",
    "# Model prediciton\n",
    "y_pred = model.predict_label(X_test[2])\n",
    "print(\"Make: {}\".format(numbertomakedict[y_pred[0][1]]))\n",
    "print(\"Model: {}\".format(numbertomodeldict[y_pred[0][2]]))\n",
    "\n",
    "# Actual answer\n",
    "print(\"The actual answer is: {} {}\".format(numbertomakedict[y_test[0][0]], numbertomodeldict[y_test[0][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Acura',\n",
       " 1: 'Mazda',\n",
       " 2: 'Chevrolet',\n",
       " 3: 'BMW',\n",
       " 4: 'Honda',\n",
       " 5: 'Buick',\n",
       " 6: 'Kia',\n",
       " 7: 'Toyota',\n",
       " 8: 'Hyundai',\n",
       " 9: 'Nissan',\n",
       " 10: 'FIAT',\n",
       " 11: 'Chrysler',\n",
       " 12: 'INFINITI',\n",
       " 13: 'Ford',\n",
       " 14: 'Volkswagen',\n",
       " 15: 'Dodge',\n",
       " 16: 'Jeep',\n",
       " 17: 'Cadillac',\n",
       " 18: 'GMC',\n",
       " 19: 'Lincoln',\n",
       " 20: 'Lexus',\n",
       " 21: 'Jaguar',\n",
       " 22: 'Smart',\n",
       " 23: 'Infiniti',\n",
       " 24: 'Mercedes-Benz',\n",
       " 25: 'Mitsubishi',\n",
       " 26: 'MINI',\n",
       " 27: 'Subaru',\n",
       " 28: 'Audi',\n",
       " 29: 'Land Rover'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbertomakedict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. With make and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we'll remove the vehicle ID from the labels to keep only make and model. It might be interesting later to add the vehicle ID to the training, to account for the repetitive number of photos from different angle. We'll keep it simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>13</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     make model\n",
       "0       0     0\n",
       "1       0     0\n",
       "2       0     0\n",
       "3       0     0\n",
       "4       0     0\n",
       "5       0     0\n",
       "6       0     0\n",
       "7       0     0\n",
       "8       0     0\n",
       "9       0     0\n",
       "10      0     0\n",
       "11      0     0\n",
       "12      0     0\n",
       "13      0     0\n",
       "14      0     0\n",
       "15      0     0\n",
       "16      0     1\n",
       "17      0     1\n",
       "18      0     1\n",
       "19      0     1\n",
       "20      0     1\n",
       "21      0     1\n",
       "22      0     1\n",
       "23      0     1\n",
       "24      0     1\n",
       "25      0     1\n",
       "26      0     1\n",
       "27      0     1\n",
       "28      0     1\n",
       "29      0     1\n",
       "...   ...   ...\n",
       "4970    4   152\n",
       "4971    4   152\n",
       "4972    4   152\n",
       "4973    4   152\n",
       "4974    4   152\n",
       "4975    4   152\n",
       "4976   17    93\n",
       "4977   17    93\n",
       "4978   17    93\n",
       "4979   17    93\n",
       "4980   17    93\n",
       "4981   17    93\n",
       "4982   17    93\n",
       "4983   17    93\n",
       "4984   17    93\n",
       "4985   17    93\n",
       "4986   17    93\n",
       "4987   17    93\n",
       "4988   17    93\n",
       "4989   17    93\n",
       "4990   17    93\n",
       "4991   17    93\n",
       "4992   13   139\n",
       "4993   13   139\n",
       "4994   13   139\n",
       "4995   13   139\n",
       "4996   13   139\n",
       "4997   13   139\n",
       "4998   13   139\n",
       "4999   13   139\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_y_train = clean_y_train.drop(\"name\", axis = 1)\n",
    "clean_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset underlying graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "tf.get_default_graph().get_operations()\n",
    "\n",
    "os.chdir(\"C:/Users/mednche/Desktop/ImageRec/\") # change directory to save checkpoint of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 19999  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 203.498s\n",
      "| Adam | epoch: 010 | loss: 0.00000 -- iter: 3998/4000\n",
      "Training Step: 20000  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 203.583s\n",
      "| Adam | epoch: 010 | loss: 0.00000 -- iter: 4000/4000\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\mednche\\Desktop\\ImageRec\\carclassifier.tfl.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:C:\\Users\\mednche\\Desktop\\ImageRec\\carclassifier.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Network trained and saved as car-classifier.tfl!\n",
      "Accuracy of the model: [0.0010000000149011613]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, clean_y_train, test_size=0.2, random_state=33)\n",
    "    \n",
    "    # trainx and trainy should be numpy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Make sure the data is normalized\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "\n",
    "    ### Define our network architecture:\n",
    "    \n",
    "    # Input is a tensor: height*width images with 3 color channels (red, green and blue) \n",
    "    \n",
    "    network = input_data(shape=[None, image_height, image_width, channels],\n",
    "                         data_preprocessing=img_prep)\n",
    "    \n",
    "    # Step 1: Convolution\n",
    "    # NB: low level recognises edges and cruves, high level recognises beaks and paws \n",
    "    network = conv_2d(network, 3, 10, activation='relu') # number of filters: 3, filter size 3 \n",
    "    # need downsampling to reduce the size of the image\n",
    "    \n",
    "    # Step 2: Max pooling\n",
    "    network = max_pool_2d(network, 2) # kernel size\n",
    "                        \n",
    "    # Step 6: Fully-connected 3 node neural network\n",
    "    # Looks at all images for each class and identify the high level features in common\n",
    "    network = fully_connected(network, 2, activation='relu') # number of outputs = number of classes the model has to choose from\n",
    "    \n",
    "    network = tflearn.regression(network)\n",
    "                             \n",
    "    \n",
    "    # Wrap the network in a model object\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=1, checkpoint_path='carclassifier.tfl.ckpt')\n",
    "    \n",
    "    %timeit model.fit(X_train, y_train, batch_size=2) # batch size of 56 or even 10 is too much\n",
    "    \n",
    "    # Save model when training is complete to a file\n",
    "    model.save(\"carclassifier.tfl\")\n",
    "    \n",
    "    print(\"Network trained and saved as car-classifier.tfl!\")\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, batch_size=10)\n",
    "    # accuracy 0.0010000000149011613\n",
    "    \n",
    "    print(\"Accuracy of the model: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [1, 2, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [1, 2, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [1, 2, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict_label(X_test[100:140])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# predict values\n",
    "img\n",
    "x = np.array(img, dtype=np.float32)\n",
    "x = np.array(x).reshape(1, 250,450,3)\n",
    "\n",
    "X_test\n",
    "\n",
    "y_pred = model.predict_label(X_test[:80]) # cannot handle more (memory load)\n",
    "\n",
    "y_pred\n",
    "y_test[:5]\n",
    "\n",
    "\n",
    "im = Image.open(folder + \"/\" + train_files[0])\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Precision \n",
    "precision_score(y_test, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall_score(y_test, y_pred)\n",
    "\n",
    "# F1 score\n",
    "f1_score(y_test,y_pred)\n",
    "\n",
    "# Cohen's kappa\n",
    "cohen_kappa_score(y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
